{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import data_utils\n",
    "import download\n",
    "from scipy.stats import skew, kurtosis\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    listOfTestFiles = os.listdir(path=path)\n",
    "    train = []\n",
    "    train_labels = []\n",
    "    test = []\n",
    "    test_labels = []\n",
    "        \n",
    "        \n",
    "    print(\"Training files = \",listOfTestFiles[1:6])\n",
    "    #For collecting Training data:\n",
    "    for file in listOfTestFiles[1:6]:\n",
    "        with open(path+file,'rb') as fo:\n",
    "            dict = pickle.load(fo,encoding='bytes')\n",
    "            train.append(dict[b'data'])\n",
    "            train_labels.append(dict[b'labels'])\n",
    "\n",
    "    print(listOfTestFiles[7])\n",
    "    #for collecting Testing data\n",
    "    with open(path+listOfTestFiles[7],'rb') as fo:\n",
    "            dict = pickle.load(fo,encoding='bytes')\n",
    "            test.append(dict[b'data'])\n",
    "            test_labels.append(dict[b'labels'])\n",
    "\n",
    "    dictData = {}\n",
    "    dictData['train_data'] = np.reshape(np.array(train),newshape=(np.array(train).shape[0]*np.array(train).shape[1],np.array(train).shape[2]))\n",
    "    dictData['train_labels'] = np.reshape(np.array(train_labels),newshape=(np.array(train_labels).shape[0]*np.array(train_labels).shape[1]))\n",
    "    dictData['test_data'] = np.reshape(np.array(test),newshape=(np.array(test).shape[0]*np.array(test).shape[1],np.array(test).shape[2]))\n",
    "    dictData['test_labels'] = np.reshape(np.array(test_labels),newshape=(np.array(test_labels).shape[0]*np.array(test_labels).shape[1]))\n",
    "    return dictData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNearestNeighbour(object):\n",
    "    def __init__(self, metric='l1'):\n",
    "        \"\"\"\n",
    "        Initialize the KNN classifier.\n",
    "        :param metric: The distance metric to use ('l1', 'l2', 'cosine').\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Memorize the training data.\n",
    "        :param X: Training data of shape (N, F).\n",
    "        :param Y: Training labels of shape (N,).\n",
    "        \"\"\"\n",
    "        self.Xtr = X\n",
    "        self.Ytr = Y\n",
    "\n",
    "    def _compute_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the distance between a single test example and all training examples.\n",
    "        :param x1: A single test example of shape (F,).\n",
    "        :param x2: Training examples of shape (N, F).\n",
    "        :return: Distance of shape (N,).\n",
    "        \"\"\"\n",
    "        if self.metric == 'l1':\n",
    "            return np.sum(np.abs(x2 - x1), axis=1)\n",
    "        elif self.metric == 'l2':\n",
    "            return np.sqrt(np.sum((x2 - x1) ** 2, axis=1))\n",
    "        elif self.metric == 'cosine':\n",
    "            x1_norm = np.linalg.norm(x1)\n",
    "            x2_norms = np.linalg.norm(x2, axis=1)\n",
    "            return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n",
    "        else:\n",
    "            return np.sum(np.abs(x2 - x1), axis=1) # default L1 distance\n",
    "\n",
    "    def predict(self, X, k):\n",
    "        \"\"\"\n",
    "        Predict labels for test data.\n",
    "        :param X: Test data of shape (M, F).\n",
    "        :param k: Number of neighbors to consider.\n",
    "        :return: Predicted labels of shape (M,).\n",
    "        \"\"\"\n",
    "        if k > self.Xtr.shape[0]:\n",
    "            raise ValueError(f\"k={k} is greater than the number of training samples={self.Xtr.shape[0]}\")\n",
    "\n",
    "        test_samples = X.shape[0]\n",
    "        Ypred = np.zeros(test_samples, dtype=self.Ytr.dtype)\n",
    "\n",
    "        for i in range(test_samples):\n",
    "            print(f\"Test example = {i}\", end=\"\\r\")\n",
    "\n",
    "            # Compute distances based on the chosen metric\n",
    "            dist = self._compute_distance(X[i, :], self.Xtr)\n",
    "\n",
    "            # Find the indices of the k smallest distances\n",
    "            idx = np.argpartition(dist, k)[:k]\n",
    "\n",
    "            # Weighted voting or simple majority voting\n",
    "            label_count = np.zeros(10, dtype=np.float64)\n",
    "            for x in idx:\n",
    "                weight = 1 / (dist[x] + 1e-9)  # Avoid division by zero\n",
    "                label_count[int(self.Ytr[x])] += weight\n",
    "\n",
    "            # Assign the label with the maximum weighted vote\n",
    "            Ypred[i] = np.argmax(label_count)\n",
    "\n",
    "        return Ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten the data\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  # (50000, 32*32*3)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)    # (10000, 32*32*3)\n",
    "\n",
    "# Reshape labels to 1D\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37 99\n"
     ]
    }
   ],
   "source": [
    "# Initialize the KNN classifier\n",
    "knn = kNearestNeighbour()\n",
    "\n",
    "# Train the classifier\n",
    "knn.train(x_train_flat, y_train)\n",
    "\n",
    "# Predict the labels for a subset of test data (e.g., 100 samples)\n",
    "num_test_samples = 100  # To save time, use a small subset for testing\n",
    "y_pred = knn.predict(x_test_flat[:num_test_samples], k=20)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=9)  # CIFAR-10 has 10 classes, so max components = 10 - 1\n",
    "x_train_lda = lda.fit_transform(x_train_flat, y_train)\n",
    "x_test_lda = lda.transform(x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 30.00%\n",
      "Accuracy for k = 2: 30.00%\n",
      "Accuracy for k = 3: 34.00%\n",
      "Accuracy for k = 4: 38.00%\n",
      "Accuracy for k = 5: 38.00%\n",
      "Accuracy for k = 6: 35.00%\n",
      "Accuracy for k = 7: 40.00%\n",
      "Accuracy for k = 8: 39.00%\n",
      "Accuracy for k = 9: 40.00%\n",
      "Accuracy for k = 10: 37.00%\n",
      "Accuracy for k = 11: 38.00%\n",
      "Accuracy for k = 12: 36.00%\n",
      "Accuracy for k = 13: 36.00%\n",
      "Accuracy for k = 14: 33.00%\n",
      "Accuracy for k = 15: 36.00%\n",
      "Accuracy for k = 16: 35.00%\n",
      "Accuracy for k = 17: 34.00%\n",
      "Accuracy for k = 18: 33.00%\n",
      "Accuracy for k = 19: 37.00%\n",
      "Accuracy for k = 20: 37.00%\n",
      "Accuracy for k = 21: 37.00%\n",
      "Accuracy for k = 22: 36.00%\n",
      "Accuracy for k = 23: 37.00%\n",
      "Accuracy for k = 24: 37.00%\n",
      "Accuracy for k = 25: 36.00%\n",
      "Accuracy for k = 26: 38.00%\n",
      "Accuracy for k = 27: 37.00%\n",
      "Accuracy for k = 28: 38.00%\n",
      "Accuracy for k = 29: 38.00%\n",
      "Accuracy for k = 30: 39.00%\n",
      "Accuracy for k = 31: 41.00%\n",
      "Accuracy for k = 32: 39.00%\n",
      "Accuracy for k = 33: 40.00%\n",
      "Accuracy for k = 34: 41.00%\n",
      "Accuracy for k = 35: 41.00%\n",
      "Accuracy for k = 36: 41.00%\n",
      "Accuracy for k = 37: 40.00%\n",
      "Accuracy for k = 38: 39.00%\n",
      "Accuracy for k = 39: 40.00%\n",
      "Accuracy for k = 40: 40.00%\n",
      "Accuracy for k = 41: 41.00%\n",
      "Accuracy for k = 42: 41.00%\n",
      "Accuracy for k = 43: 42.00%\n",
      "Accuracy for k = 44: 40.00%\n",
      "Accuracy for k = 45: 39.00%\n",
      "Accuracy for k = 46: 38.00%\n",
      "Accuracy for k = 47: 39.00%\n",
      "Accuracy for k = 48: 39.00%\n",
      "Accuracy for k = 49: 38.00%\n",
      "Accuracy for k = 50: 38.00%\n",
      "Accuracy for k = 51: 38.00%\n",
      "Accuracy for k = 52: 38.00%\n",
      "Accuracy for k = 53: 38.00%\n",
      "Accuracy for k = 54: 39.00%\n",
      "Accuracy for k = 55: 39.00%\n",
      "Accuracy for k = 56: 39.00%\n",
      "Accuracy for k = 57: 39.00%\n",
      "Accuracy for k = 58: 38.00%\n",
      "Accuracy for k = 59: 38.00%\n",
      "Accuracy for k = 60: 39.00%\n",
      "Accuracy for k = 61: 40.00%\n",
      "Accuracy for k = 62: 39.00%\n",
      "Accuracy for k = 63: 39.00%\n",
      "Accuracy for k = 64: 39.00%\n",
      "Accuracy for k = 65: 39.00%\n",
      "Accuracy for k = 66: 39.00%\n",
      "Accuracy for k = 67: 39.00%\n",
      "Accuracy for k = 68: 39.00%\n",
      "Accuracy for k = 69: 39.00%\n",
      "Accuracy for k = 70: 40.00%\n",
      "Accuracy for k = 71: 40.00%\n",
      "Accuracy for k = 72: 40.00%\n",
      "Accuracy for k = 73: 40.00%\n",
      "Accuracy for k = 74: 40.00%\n",
      "\n",
      "Best k value:\n",
      "k = 43: Accuracy = 42.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier after LDA\n",
    "\n",
    "knn.train(x_train_lda, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75) \n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_lda[:num_test_samples], k=k)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 22.00%\n",
      "Accuracy for k = 2: 22.00%\n",
      "Accuracy for k = 3: 23.00%\n",
      "Accuracy for k = 4: 34.00%\n",
      "Accuracy for k = 5: 33.00%\n",
      "Accuracy for k = 6: 30.00%\n",
      "Accuracy for k = 7: 33.00%\n",
      "Accuracy for k = 8: 31.00%\n",
      "Accuracy for k = 9: 35.00%\n",
      "Accuracy for k = 10: 33.00%\n",
      "Accuracy for k = 11: 35.00%\n",
      "Accuracy for k = 12: 34.00%\n",
      "Accuracy for k = 13: 36.00%\n",
      "Accuracy for k = 14: 36.00%\n",
      "Accuracy for k = 15: 37.00%\n",
      "Accuracy for k = 16: 37.00%\n",
      "Accuracy for k = 17: 34.00%\n",
      "Accuracy for k = 18: 36.00%\n",
      "Accuracy for k = 19: 38.00%\n",
      "Accuracy for k = 20: 36.00%\n",
      "Accuracy for k = 21: 37.00%\n",
      "Accuracy for k = 22: 36.00%\n",
      "Accuracy for k = 23: 38.00%\n",
      "Accuracy for k = 24: 37.00%\n",
      "Accuracy for k = 25: 36.00%\n",
      "Accuracy for k = 26: 39.00%\n",
      "Accuracy for k = 27: 39.00%\n",
      "Accuracy for k = 28: 38.00%\n",
      "Accuracy for k = 29: 37.00%\n",
      "Accuracy for k = 30: 38.00%\n",
      "Accuracy for k = 31: 38.00%\n",
      "Accuracy for k = 32: 38.00%\n",
      "Accuracy for k = 33: 37.00%\n",
      "Accuracy for k = 34: 38.00%\n",
      "Accuracy for k = 35: 38.00%\n",
      "Accuracy for k = 36: 40.00%\n",
      "Accuracy for k = 37: 39.00%\n",
      "Accuracy for k = 38: 40.00%\n",
      "Accuracy for k = 39: 40.00%\n",
      "Accuracy for k = 40: 40.00%\n",
      "Accuracy for k = 41: 40.00%\n",
      "Accuracy for k = 42: 40.00%\n",
      "Accuracy for k = 43: 40.00%\n",
      "Accuracy for k = 44: 39.00%\n",
      "Accuracy for k = 45: 39.00%\n",
      "Accuracy for k = 46: 40.00%\n",
      "Accuracy for k = 47: 41.00%\n",
      "Accuracy for k = 48: 40.00%\n",
      "Accuracy for k = 49: 40.00%\n",
      "Accuracy for k = 50: 40.00%\n",
      "Accuracy for k = 51: 41.00%\n",
      "Accuracy for k = 52: 40.00%\n",
      "Accuracy for k = 53: 40.00%\n",
      "Accuracy for k = 54: 40.00%\n",
      "Accuracy for k = 55: 40.00%\n",
      "Accuracy for k = 56: 40.00%\n",
      "Accuracy for k = 57: 40.00%\n",
      "Accuracy for k = 58: 39.00%\n",
      "Accuracy for k = 59: 39.00%\n",
      "Accuracy for k = 60: 40.00%\n",
      "Accuracy for k = 61: 38.00%\n",
      "Accuracy for k = 62: 39.00%\n",
      "Accuracy for k = 63: 39.00%\n",
      "Accuracy for k = 64: 38.00%\n",
      "Accuracy for k = 65: 38.00%\n",
      "Accuracy for k = 66: 38.00%\n",
      "Accuracy for k = 67: 38.00%\n",
      "Accuracy for k = 68: 39.00%\n",
      "Accuracy for k = 69: 39.00%\n",
      "Accuracy for k = 70: 39.00%\n",
      "Accuracy for k = 71: 39.00%\n",
      "Accuracy for k = 72: 39.00%\n",
      "Accuracy for k = 73: 39.00%\n",
      "Accuracy for k = 74: 38.00%\n",
      "\n",
      "Best k value:\n",
      "k = 47: Accuracy = 41.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier after LDA\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_lda, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75) \n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_lda[:num_test_samples], k=k)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
