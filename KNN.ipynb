{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from skimage import exposure, filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    listOfTestFiles = os.listdir(path=path)\n",
    "    train = []\n",
    "    train_labels = []\n",
    "    test = []\n",
    "    test_labels = []\n",
    "        \n",
    "        \n",
    "    print(\"Training files = \",listOfTestFiles[1:6])\n",
    "    #For collecting Training data:\n",
    "    for file in listOfTestFiles[1:6]:\n",
    "        with open(path+file,'rb') as fo:\n",
    "            dict = pickle.load(fo,encoding='bytes')\n",
    "            train.append(dict[b'data'])\n",
    "            train_labels.append(dict[b'labels'])\n",
    "\n",
    "    print(listOfTestFiles[7])\n",
    "    #for collecting Testing data\n",
    "    with open(path+listOfTestFiles[7],'rb') as fo:\n",
    "            dict = pickle.load(fo,encoding='bytes')\n",
    "            test.append(dict[b'data'])\n",
    "            test_labels.append(dict[b'labels'])\n",
    "\n",
    "    dictData = {}\n",
    "    dictData['train_data'] = np.reshape(np.array(train),newshape=(np.array(train).shape[0]*np.array(train).shape[1],np.array(train).shape[2]))\n",
    "    dictData['train_labels'] = np.reshape(np.array(train_labels),newshape=(np.array(train_labels).shape[0]*np.array(train_labels).shape[1]))\n",
    "    dictData['test_data'] = np.reshape(np.array(test),newshape=(np.array(test).shape[0]*np.array(test).shape[1],np.array(test).shape[2]))\n",
    "    dictData['test_labels'] = np.reshape(np.array(test_labels),newshape=(np.array(test_labels).shape[0]*np.array(test_labels).shape[1]))\n",
    "    return dictData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNearestNeighbour(object):\n",
    "    def __init__(self, metric='l1'):\n",
    "        \"\"\"\n",
    "        Initialize the KNN classifier.\n",
    "        :param metric: The distance metric to use ('l1', 'l2', 'cosine').\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Memorize the training data.\n",
    "        :param X: Training data of shape (N, F).\n",
    "        :param Y: Training labels of shape (N,).\n",
    "        \"\"\"\n",
    "        self.Xtr = X\n",
    "        self.Ytr = Y\n",
    "\n",
    "    def _compute_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the distance between a single test example and all training examples.\n",
    "        :param x1: A single test example of shape (F,).\n",
    "        :param x2: Training examples of shape (N, F).\n",
    "        :return: Distance of shape (N,).\n",
    "        \"\"\"\n",
    "        if self.metric == 'l1':\n",
    "            return np.sum(np.abs(x2 - x1), axis=1)\n",
    "        elif self.metric == 'l2':\n",
    "            return np.sqrt(np.sum((x2 - x1) ** 2, axis=1))\n",
    "        elif self.metric == 'cosine':\n",
    "            x1_norm = np.linalg.norm(x1)\n",
    "            x2_norms = np.linalg.norm(x2, axis=1)\n",
    "            return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n",
    "        else:\n",
    "            return np.sum(np.abs(x2 - x1), axis=1) # default L1 distance\n",
    "\n",
    "    def predict(self, X, k):\n",
    "        \"\"\"\n",
    "        Predict labels for test data.\n",
    "        :param X: Test data of shape (M, F).\n",
    "        :param k: Number of neighbors to consider.\n",
    "        :return: Predicted labels of shape (M,).\n",
    "        \"\"\"\n",
    "        if k > self.Xtr.shape[0]:\n",
    "            raise ValueError(f\"k={k} is greater than the number of training samples={self.Xtr.shape[0]}\")\n",
    "\n",
    "        test_samples = X.shape[0]\n",
    "        Ypred = np.zeros(test_samples, dtype=self.Ytr.dtype)\n",
    "\n",
    "        for i in range(test_samples):\n",
    "            print(f\"Test example = {i}\", end=\"\\r\")\n",
    "\n",
    "            # Compute distances based on the chosen metric\n",
    "            dist = self._compute_distance(X[i, :], self.Xtr)\n",
    "\n",
    "            # Find the indices of the k smallest distances\n",
    "            idx = np.argpartition(dist, k)[:k]\n",
    "\n",
    "            # Weighted voting or simple majority voting\n",
    "            label_count = np.zeros(10, dtype=np.float64)\n",
    "            for x in idx:\n",
    "                weight = 1 / (dist[x] + 1e-9)  # Avoid division by zero\n",
    "                label_count[int(self.Ytr[x])] += weight\n",
    "\n",
    "            # Assign the label with the maximum weighted vote\n",
    "            Ypred[i] = np.argmax(label_count)\n",
    "\n",
    "        return Ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten the data\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  # (50000, 32*32*3)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)    # (10000, 32*32*3)\n",
    "\n",
    "# Reshape labels to 1D\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37 99\n"
     ]
    }
   ],
   "source": [
    "# Initialize the KNN classifier\n",
    "knn = kNearestNeighbour()\n",
    "\n",
    "# Train the classifier\n",
    "knn.train(x_train_flat, y_train)\n",
    "\n",
    "# Predict the labels for a subset of test data (e.g., 100 samples)\n",
    "num_test_samples = 100  # To save time, use a small subset for testing\n",
    "y_pred = knn.predict(x_test_flat[:num_test_samples], k=20)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=9)  # CIFAR-10 has 10 classes, so max components = 10 - 1\n",
    "x_train_lda = lda.fit_transform(x_train_flat, y_train)\n",
    "x_test_lda = lda.transform(x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 30.00%\n",
      "Accuracy for k = 2: 30.00%\n",
      "Accuracy for k = 3: 34.00%\n",
      "Accuracy for k = 4: 38.00%\n",
      "Accuracy for k = 5: 38.00%\n",
      "Accuracy for k = 6: 35.00%\n",
      "Accuracy for k = 7: 40.00%\n",
      "Accuracy for k = 8: 39.00%\n",
      "Accuracy for k = 9: 40.00%\n",
      "Accuracy for k = 10: 37.00%\n",
      "Accuracy for k = 11: 38.00%\n",
      "Accuracy for k = 12: 36.00%\n",
      "Accuracy for k = 13: 36.00%\n",
      "Accuracy for k = 14: 33.00%\n",
      "Accuracy for k = 15: 36.00%\n",
      "Accuracy for k = 16: 35.00%\n",
      "Accuracy for k = 17: 34.00%\n",
      "Accuracy for k = 18: 33.00%\n",
      "Accuracy for k = 19: 37.00%\n",
      "Accuracy for k = 20: 37.00%\n",
      "Accuracy for k = 21: 37.00%\n",
      "Accuracy for k = 22: 36.00%\n",
      "Accuracy for k = 23: 37.00%\n",
      "Accuracy for k = 24: 37.00%\n",
      "Accuracy for k = 25: 36.00%\n",
      "Accuracy for k = 26: 38.00%\n",
      "Accuracy for k = 27: 37.00%\n",
      "Accuracy for k = 28: 38.00%\n",
      "Accuracy for k = 29: 38.00%\n",
      "Accuracy for k = 30: 39.00%\n",
      "Accuracy for k = 31: 41.00%\n",
      "Accuracy for k = 32: 39.00%\n",
      "Accuracy for k = 33: 40.00%\n",
      "Accuracy for k = 34: 41.00%\n",
      "Accuracy for k = 35: 41.00%\n",
      "Accuracy for k = 36: 41.00%\n",
      "Accuracy for k = 37: 40.00%\n",
      "Accuracy for k = 38: 39.00%\n",
      "Accuracy for k = 39: 40.00%\n",
      "Accuracy for k = 40: 40.00%\n",
      "Accuracy for k = 41: 41.00%\n",
      "Accuracy for k = 42: 41.00%\n",
      "Accuracy for k = 43: 42.00%\n",
      "Accuracy for k = 44: 40.00%\n",
      "Accuracy for k = 45: 39.00%\n",
      "Accuracy for k = 46: 38.00%\n",
      "Accuracy for k = 47: 39.00%\n",
      "Accuracy for k = 48: 39.00%\n",
      "Accuracy for k = 49: 38.00%\n",
      "Accuracy for k = 50: 38.00%\n",
      "Accuracy for k = 51: 38.00%\n",
      "Accuracy for k = 52: 38.00%\n",
      "Accuracy for k = 53: 38.00%\n",
      "Accuracy for k = 54: 39.00%\n",
      "Accuracy for k = 55: 39.00%\n",
      "Accuracy for k = 56: 39.00%\n",
      "Accuracy for k = 57: 39.00%\n",
      "Accuracy for k = 58: 38.00%\n",
      "Accuracy for k = 59: 38.00%\n",
      "Accuracy for k = 60: 39.00%\n",
      "Accuracy for k = 61: 40.00%\n",
      "Accuracy for k = 62: 39.00%\n",
      "Accuracy for k = 63: 39.00%\n",
      "Accuracy for k = 64: 39.00%\n",
      "Accuracy for k = 65: 39.00%\n",
      "Accuracy for k = 66: 39.00%\n",
      "Accuracy for k = 67: 39.00%\n",
      "Accuracy for k = 68: 39.00%\n",
      "Accuracy for k = 69: 39.00%\n",
      "Accuracy for k = 70: 40.00%\n",
      "Accuracy for k = 71: 40.00%\n",
      "Accuracy for k = 72: 40.00%\n",
      "Accuracy for k = 73: 40.00%\n",
      "Accuracy for k = 74: 40.00%\n",
      "\n",
      "Best k value:\n",
      "k = 43: Accuracy = 42.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier after LDA\n",
    "\n",
    "knn.train(x_train_lda, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75) \n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_lda[:num_test_samples], k=k)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 22.00%\n",
      "Accuracy for k = 2: 22.00%\n",
      "Accuracy for k = 3: 27.00%\n",
      "Accuracy for k = 4: 33.00%\n",
      "Accuracy for k = 5: 33.00%\n",
      "Accuracy for k = 6: 36.00%\n",
      "Accuracy for k = 7: 35.00%\n",
      "Accuracy for k = 8: 35.00%\n",
      "Accuracy for k = 9: 37.00%\n",
      "Accuracy for k = 10: 37.00%\n",
      "Accuracy for k = 11: 35.00%\n",
      "Accuracy for k = 12: 35.00%\n",
      "Accuracy for k = 13: 36.00%\n",
      "Accuracy for k = 14: 38.00%\n",
      "Accuracy for k = 15: 40.00%\n",
      "Accuracy for k = 16: 37.00%\n",
      "Accuracy for k = 17: 40.00%\n",
      "Accuracy for k = 18: 41.00%\n",
      "Accuracy for k = 19: 41.00%\n",
      "Accuracy for k = 20: 38.00%\n",
      "Accuracy for k = 21: 40.00%\n",
      "Accuracy for k = 22: 39.00%\n",
      "Accuracy for k = 23: 40.00%\n",
      "Accuracy for k = 24: 41.00%\n",
      "Accuracy for k = 25: 41.00%\n",
      "Accuracy for k = 26: 41.00%\n",
      "Accuracy for k = 27: 40.00%\n",
      "Accuracy for k = 28: 41.00%\n",
      "Accuracy for k = 29: 41.00%\n",
      "Accuracy for k = 30: 41.00%\n",
      "Accuracy for k = 31: 41.00%\n",
      "Accuracy for k = 32: 41.00%\n",
      "Accuracy for k = 33: 41.00%\n",
      "Accuracy for k = 34: 41.00%\n",
      "Accuracy for k = 35: 41.00%\n",
      "Accuracy for k = 36: 41.00%\n",
      "Accuracy for k = 37: 41.00%\n",
      "Accuracy for k = 38: 41.00%\n",
      "Accuracy for k = 39: 41.00%\n",
      "Accuracy for k = 40: 41.00%\n",
      "Accuracy for k = 41: 41.00%\n",
      "Accuracy for k = 42: 40.00%\n",
      "Accuracy for k = 43: 41.00%\n",
      "Accuracy for k = 44: 41.00%\n",
      "Accuracy for k = 45: 41.00%\n",
      "Accuracy for k = 46: 41.00%\n",
      "Accuracy for k = 47: 41.00%\n",
      "Accuracy for k = 48: 40.00%\n",
      "Accuracy for k = 49: 41.00%\n",
      "Accuracy for k = 50: 41.00%\n",
      "Accuracy for k = 51: 42.00%\n",
      "Accuracy for k = 52: 42.00%\n",
      "Accuracy for k = 53: 41.00%\n",
      "Accuracy for k = 54: 41.00%\n",
      "Accuracy for k = 55: 42.00%\n",
      "Accuracy for k = 56: 42.00%\n",
      "Accuracy for k = 57: 42.00%\n",
      "Accuracy for k = 58: 43.00%\n",
      "Accuracy for k = 59: 43.00%\n",
      "Accuracy for k = 60: 43.00%\n",
      "Accuracy for k = 61: 43.00%\n",
      "Accuracy for k = 62: 44.00%\n",
      "Accuracy for k = 63: 44.00%\n",
      "Accuracy for k = 64: 44.00%\n",
      "Accuracy for k = 65: 43.00%\n",
      "Accuracy for k = 66: 44.00%\n",
      "Accuracy for k = 67: 44.00%\n",
      "Accuracy for k = 68: 44.00%\n",
      "Accuracy for k = 69: 44.00%\n",
      "Accuracy for k = 70: 44.00%\n",
      "Accuracy for k = 71: 44.00%\n",
      "Accuracy for k = 72: 43.00%\n",
      "Accuracy for k = 73: 42.00%\n",
      "Accuracy for k = 74: 43.00%\n",
      "\n",
      "Best k value:\n",
      "k = 62: Accuracy = 44.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier after LDA\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_lda, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75) \n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_lda[:num_test_samples], k=k)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "n_components = 100  # Set the number of components you want to keep after PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA on training data and transform both train and test data\n",
    "x_train_pca = pca.fit_transform(x_train_flat)\n",
    "x_test_pca = pca.transform(x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 38.00%\n",
      "Accuracy for k = 2: 38.00%\n",
      "Accuracy for k = 3: 40.00%\n",
      "Accuracy for k = 4: 43.00%\n",
      "Accuracy for k = 5: 40.00%\n",
      "Accuracy for k = 6: 38.00%\n",
      "Accuracy for k = 7: 42.00%\n",
      "Accuracy for k = 8: 43.00%\n",
      "Accuracy for k = 9: 45.00%\n",
      "Accuracy for k = 10: 41.00%\n",
      "Accuracy for k = 11: 43.00%\n",
      "Accuracy for k = 12: 46.00%\n",
      "Accuracy for k = 13: 44.00%\n",
      "Accuracy for k = 14: 47.00%\n",
      "Accuracy for k = 15: 48.00%\n",
      "Accuracy for k = 16: 50.00%\n",
      "Accuracy for k = 17: 47.00%\n",
      "Accuracy for k = 18: 46.00%\n",
      "Accuracy for k = 19: 49.00%\n",
      "Accuracy for k = 20: 46.00%\n",
      "Accuracy for k = 21: 49.00%\n",
      "Accuracy for k = 22: 48.00%\n",
      "Accuracy for k = 23: 50.00%\n",
      "Accuracy for k = 24: 53.00%\n",
      "Accuracy for k = 25: 52.00%\n",
      "Accuracy for k = 26: 52.00%\n",
      "Accuracy for k = 27: 50.00%\n",
      "Accuracy for k = 28: 50.00%\n",
      "Accuracy for k = 29: 50.00%\n",
      "Accuracy for k = 30: 50.00%\n",
      "Accuracy for k = 31: 51.00%\n",
      "Accuracy for k = 32: 51.00%\n",
      "Accuracy for k = 33: 50.00%\n",
      "Accuracy for k = 34: 50.00%\n",
      "Accuracy for k = 35: 51.00%\n",
      "Accuracy for k = 36: 53.00%\n",
      "Accuracy for k = 37: 51.00%\n",
      "Accuracy for k = 38: 49.00%\n",
      "Accuracy for k = 39: 48.00%\n",
      "Accuracy for k = 40: 47.00%\n",
      "Accuracy for k = 41: 49.00%\n",
      "Accuracy for k = 42: 48.00%\n",
      "Accuracy for k = 43: 49.00%\n",
      "Accuracy for k = 44: 49.00%\n",
      "Accuracy for k = 45: 45.00%\n",
      "Accuracy for k = 46: 46.00%\n",
      "Accuracy for k = 47: 48.00%\n",
      "Accuracy for k = 48: 47.00%\n",
      "Accuracy for k = 49: 45.00%\n",
      "Accuracy for k = 50: 45.00%\n",
      "Accuracy for k = 51: 47.00%\n",
      "Accuracy for k = 52: 48.00%\n",
      "Accuracy for k = 53: 50.00%\n",
      "Accuracy for k = 54: 49.00%\n",
      "Accuracy for k = 55: 46.00%\n",
      "Accuracy for k = 56: 45.00%\n",
      "Accuracy for k = 57: 45.00%\n",
      "Accuracy for k = 58: 44.00%\n",
      "Accuracy for k = 59: 45.00%\n",
      "Accuracy for k = 60: 46.00%\n",
      "Accuracy for k = 61: 45.00%\n",
      "Accuracy for k = 62: 43.00%\n",
      "Accuracy for k = 63: 45.00%\n",
      "Accuracy for k = 64: 43.00%\n",
      "Accuracy for k = 65: 43.00%\n",
      "Accuracy for k = 66: 44.00%\n",
      "Accuracy for k = 67: 44.00%\n",
      "Accuracy for k = 68: 43.00%\n",
      "Accuracy for k = 69: 45.00%\n",
      "Accuracy for k = 70: 45.00%\n",
      "Accuracy for k = 71: 47.00%\n",
      "Accuracy for k = 72: 45.00%\n",
      "Accuracy for k = 73: 46.00%\n",
      "Accuracy for k = 74: 46.00%\n",
      "\n",
      "Best k value:\n",
      "k = 24: Accuracy = 53.00%\n"
     ]
    }
   ],
   "source": [
    "# Flatten y_test if necessary\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Train the classifier after PCA\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_pca, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75)\n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_pca[:num_test_samples], k=k)  # Use x_test_pca here\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_features(images, kernels=None):\n",
    "    \"\"\"\n",
    "    Apply Gabor filters to a batch of images.\n",
    "    \n",
    "    Parameters:\n",
    "    - images: A batch of images of shape (N, H, W, C) where N is the number of images,\n",
    "      H is height, W is width, and C is the number of channels.\n",
    "    - kernels: A list of Gabor kernels to apply. If None, it generates a set of kernels.\n",
    "    \n",
    "    Returns:\n",
    "    - features: A 2D array of shape (N, F) where F is the number of features per image.\n",
    "    \"\"\"\n",
    "    if kernels is None:\n",
    "        kernels = generate_gabor_kernels()\n",
    "    \n",
    "    # Initialize a list to store the features\n",
    "    features = []\n",
    "    \n",
    "    for img in images:\n",
    "        # Ensure the image is grayscale (Gabor requires single-channel images)\n",
    "        if img.ndim == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        img_features = []\n",
    "        for kernel in kernels:\n",
    "            # Apply the Gabor filter\n",
    "            filtered = cv2.filter2D(img, cv2.CV_32F, kernel)\n",
    "            # Extract statistics as features (mean and standard deviation)\n",
    "            img_features.append(filtered.mean())\n",
    "            img_features.append(filtered.std())\n",
    "        \n",
    "        features.append(img_features)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def generate_gabor_kernels(scales=5, orientations=8):\n",
    "    \"\"\"\n",
    "    Generate Gabor filter kernels.\n",
    "    \n",
    "    Parameters:\n",
    "    - scales: Number of different scales (frequencies).\n",
    "    - orientations: Number of orientations for each scale.\n",
    "    \n",
    "    Returns:\n",
    "    - kernels: A list of Gabor kernels.\n",
    "    \"\"\"\n",
    "    kernels = []\n",
    "    for theta in np.linspace(0, np.pi, orientations, endpoint=False):  # Orientations\n",
    "        for sigma in (1, 2):  # Scale parameters\n",
    "            kernel = cv2.getGaborKernel(ksize=(9, 9), sigma=sigma, theta=theta, lambd=10.0, gamma=0.5, psi=0)\n",
    "            kernels.append(kernel)\n",
    "    return kernels\n",
    "def generate_features_in_batches(data, batch_size, feature_extractor):\n",
    "    \"\"\"\n",
    "    Generate features in batches to save memory.\n",
    "    :param data: Original data of shape (N, H, W, C).\n",
    "    :param batch_size: Batch size for feature computation.\n",
    "    :param feature_extractor: A callable that extracts features for a batch.\n",
    "    :return: Features array.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        print(f\"Processing batch {i // batch_size + 1}\", end=\"\\r\")\n",
    "        batch = data[i:i + batch_size]\n",
    "        batch_features = feature_extractor(batch)  # Apply feature extraction\n",
    "        features.append(batch_features)\n",
    "    return np.vstack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10\r"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_gabor = generate_features_in_batches(x_train, batch_size=1000, feature_extractor=gabor_features)\n",
    "x_test_gabor = generate_features_in_batches(x_test, batch_size=1000, feature_extractor=gabor_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phog(image):\n",
    "    fd, _ = hog(image, orientations=9, pixels_per_cell=(4, 4), cells_per_block=(1, 1), visualize=True)\n",
    "    return fd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size=(16, 16)):\n",
    "    return resize(image, target_size, mode='reflect', anti_aliasing=True)\n",
    "\n",
    "# Resize Gabor-filtered training and test data\n",
    "x_train_gabor_resized = np.array([resize_image(img) for img in x_train_gabor])\n",
    "x_test_gabor_resized = np.array([resize_image(img) for img in x_test_gabor])\n",
    "\n",
    "# Compute PHOG features\n",
    "x_train_phog = np.array([compute_phog(img) for img in x_train_gabor_resized])\n",
    "x_test_phog = np.array([compute_phog(img) for img in x_test_gabor_resized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)  # Adjust the number of components based on experimentation\n",
    "x_train_phog_pca = pca.fit_transform(x_train_phog)\n",
    "x_test_phog_pca = pca.transform(x_test_phog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test example = 9\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 16.00%\n",
      "Test example = 8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 2: 16.00%\n",
      "Test example = 0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 3: 16.00%\n",
      "Test example = 8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 4: 7.00%\n",
      "Test example = 4\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 5: 6.00%\n",
      "Test example = 2\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 6: 6.00%\n",
      "Test example = 4\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 7: 6.00%\n",
      "Test example = 6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 8: 6.00%\n",
      "Test example = 5\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 9: 6.00%\n",
      "Test example = 3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 10: 6.00%\n",
      "Test example = 4\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 11: 6.00%\n",
      "Test example = 7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 12: 6.00%\n",
      "Test example = 3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 13: 6.00%\n",
      "Test example = 4\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 14: 6.00%\n",
      "Test example = 3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 15: 6.00%\n",
      "Test example = 0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 16: 6.00%\n",
      "Test example = 0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kou\\AppData\\Local\\Temp\\ipykernel_13552\\1304169510.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test example = 65\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Predict the labels for a subset of test data\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     num_test_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# To save time, use a small subset for testing\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_phog_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_test_samples\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use x_test_pca here\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test[:num_test_samples], y_pred)\n",
      "Cell \u001b[1;32mIn[81], line 53\u001b[0m, in \u001b[0;36mkNearestNeighbour.predict\u001b[1;34m(self, X, k)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest example = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Compute distances based on the chosen metric\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXtr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Find the indices of the k smallest distances\u001b[39;00m\n\u001b[0;32m     56\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(dist, k)[:k]\n",
      "Cell \u001b[1;32mIn[81], line 31\u001b[0m, in \u001b[0;36mkNearestNeighbour._compute_distance\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     30\u001b[0m     x1_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x1)\n\u001b[1;32m---> 31\u001b[0m     x2_norms \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (np\u001b[38;5;241m.\u001b[39mdot(x2, x1) \u001b[38;5;241m/\u001b[39m (x1_norm \u001b[38;5;241m*\u001b[39m x2_norms))\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\github\\TP-final-IRDP\\venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2773\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2770\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2771\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[0;32m   2772\u001b[0m     s \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mconj() \u001b[38;5;241m*\u001b[39m x)\u001b[38;5;241m.\u001b[39mreal\n\u001b[1;32m-> 2773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add\u001b[38;5;241m.\u001b[39mreduce(s, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims))\n\u001b[0;32m   2774\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[0;32m   2775\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n\u001b[0;32m   2776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mord\u001b[39m, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Flatten y_test if necessary\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Train the classifier after PCA\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_phog_pca, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75)\n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_phog_pca[:num_test_samples], k=k)  # Use x_test_pca here\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
