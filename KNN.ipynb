{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage.exposure import equalize_hist\n",
    "from skimage import exposure, filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    listOfTestFiles = os.listdir(path=path)\n",
    "    train = []\n",
    "    train_labels = []\n",
    "    test = []\n",
    "    test_labels = []\n",
    "        \n",
    "        \n",
    "    print(\"Training files = \",listOfTestFiles[1:6])\n",
    "    #For collecting Training data:\n",
    "    for file in listOfTestFiles[1:6]:\n",
    "        with open(path+file,'rb') as fo:\n",
    "            dict = pickle.load(fo,encoding='bytes')\n",
    "            train.append(dict[b'data'])\n",
    "            train_labels.append(dict[b'labels'])\n",
    "\n",
    "    print(listOfTestFiles[7])\n",
    "    #for collecting Testing data\n",
    "    with open(path+listOfTestFiles[7],'rb') as fo:\n",
    "            dict = pickle.load(fo,encoding='bytes')\n",
    "            test.append(dict[b'data'])\n",
    "            test_labels.append(dict[b'labels'])\n",
    "\n",
    "    dictData = {}\n",
    "    dictData['train_data'] = np.reshape(np.array(train),newshape=(np.array(train).shape[0]*np.array(train).shape[1],np.array(train).shape[2]))\n",
    "    dictData['train_labels'] = np.reshape(np.array(train_labels),newshape=(np.array(train_labels).shape[0]*np.array(train_labels).shape[1]))\n",
    "    dictData['test_data'] = np.reshape(np.array(test),newshape=(np.array(test).shape[0]*np.array(test).shape[1],np.array(test).shape[2]))\n",
    "    dictData['test_labels'] = np.reshape(np.array(test_labels),newshape=(np.array(test_labels).shape[0]*np.array(test_labels).shape[1]))\n",
    "    return dictData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNearestNeighbour(object):\n",
    "    def __init__(self, metric='l1'):\n",
    "        \"\"\"\n",
    "        Initialize the KNN classifier.\n",
    "        :param metric: The distance metric to use ('l1', 'l2', 'cosine').\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Memorize the training data.\n",
    "        :param X: Training data of shape (N, F).\n",
    "        :param Y: Training labels of shape (N,).\n",
    "        \"\"\"\n",
    "        self.Xtr = X\n",
    "        self.Ytr = Y\n",
    "\n",
    "    def _compute_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the distance between a single test example and all training examples.\n",
    "        :param x1: A single test example of shape (F,).\n",
    "        :param x2: Training examples of shape (N, F).\n",
    "        :return: Distance of shape (N,).\n",
    "        \"\"\"\n",
    "        if self.metric == 'l1':\n",
    "            return np.sum(np.abs(x2 - x1), axis=1)\n",
    "        elif self.metric == 'l2':\n",
    "            return np.sqrt(np.sum((x2 - x1) ** 2, axis=1))\n",
    "        elif self.metric == 'cosine':\n",
    "            x1_norm = np.linalg.norm(x1)\n",
    "            x2_norms = np.linalg.norm(x2, axis=1)\n",
    "            return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n",
    "        else:\n",
    "            return np.sum(np.abs(x2 - x1), axis=1) # default L1 distance\n",
    "\n",
    "    def predict(self, X, k):\n",
    "        \"\"\"\n",
    "        Predict labels for test data.\n",
    "        :param X: Test data of shape (M, F).\n",
    "        :param k: Number of neighbors to consider.\n",
    "        :return: Predicted labels of shape (M,).\n",
    "        \"\"\"\n",
    "        if k > self.Xtr.shape[0]:\n",
    "            raise ValueError(f\"k={k} is greater than the number of training samples={self.Xtr.shape[0]}\")\n",
    "\n",
    "        test_samples = X.shape[0]\n",
    "        Ypred = np.zeros(test_samples, dtype=self.Ytr.dtype)\n",
    "\n",
    "        for i in range(test_samples):\n",
    "            print(f\"Test example = {i}\", end=\"\\r\")\n",
    "\n",
    "            # Compute distances based on the chosen metric\n",
    "            dist = self._compute_distance(X[i, :], self.Xtr)\n",
    "\n",
    "            # Find the indices of the k smallest distances\n",
    "            idx = np.argpartition(dist, k)[:k]\n",
    "\n",
    "            # Weighted voting or simple majority voting\n",
    "            label_count = np.zeros(10, dtype=np.float64)\n",
    "            for x in idx:\n",
    "                weight = 1 / (dist[x] + 1e-9)  # Avoid division by zero\n",
    "                label_count[int(self.Ytr[x])] += weight\n",
    "\n",
    "            # Assign the label with the maximum weighted vote\n",
    "            Ypred[i] = np.argmax(label_count)\n",
    "\n",
    "        return Ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten the data\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  # (50000, 32*32*3)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)    # (10000, 32*32*3)\n",
    "\n",
    "# Reshape labels to 1D\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37 99\n"
     ]
    }
   ],
   "source": [
    "# Initialize the KNN classifier\n",
    "knn = kNearestNeighbour()\n",
    "\n",
    "# Train the classifier\n",
    "knn.train(x_train_flat, y_train)\n",
    "\n",
    "# Predict the labels for a subset of test data (e.g., 100 samples)\n",
    "num_test_samples = 100  # To save time, use a small subset for testing\n",
    "y_pred = knn.predict(x_test_flat[:num_test_samples], k=20)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=9)  # CIFAR-10 has 10 classes, so max components = 10 - 1\n",
    "x_train_lda = lda.fit_transform(x_train_flat, y_train)\n",
    "x_test_lda = lda.transform(x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 30.00%\n",
      "Accuracy for k = 2: 30.00%\n",
      "Accuracy for k = 3: 34.00%\n",
      "Accuracy for k = 4: 38.00%\n",
      "Accuracy for k = 5: 38.00%\n",
      "Accuracy for k = 6: 35.00%\n",
      "Accuracy for k = 7: 40.00%\n",
      "Accuracy for k = 8: 39.00%\n",
      "Accuracy for k = 9: 40.00%\n",
      "Accuracy for k = 10: 37.00%\n",
      "Accuracy for k = 11: 38.00%\n",
      "Accuracy for k = 12: 36.00%\n",
      "Accuracy for k = 13: 36.00%\n",
      "Accuracy for k = 14: 33.00%\n",
      "Accuracy for k = 15: 36.00%\n",
      "Accuracy for k = 16: 35.00%\n",
      "Accuracy for k = 17: 34.00%\n",
      "Accuracy for k = 18: 33.00%\n",
      "Accuracy for k = 19: 37.00%\n",
      "Accuracy for k = 20: 37.00%\n",
      "Accuracy for k = 21: 37.00%\n",
      "Accuracy for k = 22: 36.00%\n",
      "Accuracy for k = 23: 37.00%\n",
      "Accuracy for k = 24: 37.00%\n",
      "Accuracy for k = 25: 36.00%\n",
      "Accuracy for k = 26: 38.00%\n",
      "Accuracy for k = 27: 37.00%\n",
      "Accuracy for k = 28: 38.00%\n",
      "Accuracy for k = 29: 38.00%\n",
      "Accuracy for k = 30: 39.00%\n",
      "Accuracy for k = 31: 41.00%\n",
      "Accuracy for k = 32: 39.00%\n",
      "Accuracy for k = 33: 40.00%\n",
      "Accuracy for k = 34: 41.00%\n",
      "Accuracy for k = 35: 41.00%\n",
      "Accuracy for k = 36: 41.00%\n",
      "Accuracy for k = 37: 40.00%\n",
      "Accuracy for k = 38: 39.00%\n",
      "Accuracy for k = 39: 40.00%\n",
      "Accuracy for k = 40: 40.00%\n",
      "Accuracy for k = 41: 41.00%\n",
      "Accuracy for k = 42: 41.00%\n",
      "Accuracy for k = 43: 42.00%\n",
      "Accuracy for k = 44: 40.00%\n",
      "Accuracy for k = 45: 39.00%\n",
      "Accuracy for k = 46: 38.00%\n",
      "Accuracy for k = 47: 39.00%\n",
      "Accuracy for k = 48: 39.00%\n",
      "Accuracy for k = 49: 38.00%\n",
      "Accuracy for k = 50: 38.00%\n",
      "Accuracy for k = 51: 38.00%\n",
      "Accuracy for k = 52: 38.00%\n",
      "Accuracy for k = 53: 38.00%\n",
      "Accuracy for k = 54: 39.00%\n",
      "Accuracy for k = 55: 39.00%\n",
      "Accuracy for k = 56: 39.00%\n",
      "Accuracy for k = 57: 39.00%\n",
      "Accuracy for k = 58: 38.00%\n",
      "Accuracy for k = 59: 38.00%\n",
      "Accuracy for k = 60: 39.00%\n",
      "Accuracy for k = 61: 40.00%\n",
      "Accuracy for k = 62: 39.00%\n",
      "Accuracy for k = 63: 39.00%\n",
      "Accuracy for k = 64: 39.00%\n",
      "Accuracy for k = 65: 39.00%\n",
      "Accuracy for k = 66: 39.00%\n",
      "Accuracy for k = 67: 39.00%\n",
      "Accuracy for k = 68: 39.00%\n",
      "Accuracy for k = 69: 39.00%\n",
      "Accuracy for k = 70: 40.00%\n",
      "Accuracy for k = 71: 40.00%\n",
      "Accuracy for k = 72: 40.00%\n",
      "Accuracy for k = 73: 40.00%\n",
      "Accuracy for k = 74: 40.00%\n",
      "\n",
      "Best k value:\n",
      "k = 43: Accuracy = 42.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier after LDA\n",
    "\n",
    "knn.train(x_train_lda, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75) \n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_lda[:num_test_samples], k=k)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 22.00%\n",
      "Accuracy for k = 2: 22.00%\n",
      "Accuracy for k = 3: 27.00%\n",
      "Accuracy for k = 4: 33.00%\n",
      "Accuracy for k = 5: 33.00%\n",
      "Accuracy for k = 6: 36.00%\n",
      "Accuracy for k = 7: 35.00%\n",
      "Accuracy for k = 8: 35.00%\n",
      "Accuracy for k = 9: 37.00%\n",
      "Accuracy for k = 10: 37.00%\n",
      "Accuracy for k = 11: 35.00%\n",
      "Accuracy for k = 12: 35.00%\n",
      "Accuracy for k = 13: 36.00%\n",
      "Accuracy for k = 14: 38.00%\n",
      "Accuracy for k = 15: 40.00%\n",
      "Accuracy for k = 16: 37.00%\n",
      "Accuracy for k = 17: 40.00%\n",
      "Accuracy for k = 18: 41.00%\n",
      "Accuracy for k = 19: 41.00%\n",
      "Accuracy for k = 20: 38.00%\n",
      "Accuracy for k = 21: 40.00%\n",
      "Accuracy for k = 22: 39.00%\n",
      "Accuracy for k = 23: 40.00%\n",
      "Accuracy for k = 24: 41.00%\n",
      "Accuracy for k = 25: 41.00%\n",
      "Accuracy for k = 26: 41.00%\n",
      "Accuracy for k = 27: 40.00%\n",
      "Accuracy for k = 28: 41.00%\n",
      "Accuracy for k = 29: 41.00%\n",
      "Accuracy for k = 30: 41.00%\n",
      "Accuracy for k = 31: 41.00%\n",
      "Accuracy for k = 32: 41.00%\n",
      "Accuracy for k = 33: 41.00%\n",
      "Accuracy for k = 34: 41.00%\n",
      "Accuracy for k = 35: 41.00%\n",
      "Accuracy for k = 36: 41.00%\n",
      "Accuracy for k = 37: 41.00%\n",
      "Accuracy for k = 38: 41.00%\n",
      "Accuracy for k = 39: 41.00%\n",
      "Accuracy for k = 40: 41.00%\n",
      "Accuracy for k = 41: 41.00%\n",
      "Accuracy for k = 42: 40.00%\n",
      "Accuracy for k = 43: 41.00%\n",
      "Accuracy for k = 44: 41.00%\n",
      "Accuracy for k = 45: 41.00%\n",
      "Accuracy for k = 46: 41.00%\n",
      "Accuracy for k = 47: 41.00%\n",
      "Accuracy for k = 48: 40.00%\n",
      "Accuracy for k = 49: 41.00%\n",
      "Accuracy for k = 50: 41.00%\n",
      "Accuracy for k = 51: 42.00%\n",
      "Accuracy for k = 52: 42.00%\n",
      "Accuracy for k = 53: 41.00%\n",
      "Accuracy for k = 54: 41.00%\n",
      "Accuracy for k = 55: 42.00%\n",
      "Accuracy for k = 56: 42.00%\n",
      "Accuracy for k = 57: 42.00%\n",
      "Accuracy for k = 58: 43.00%\n",
      "Accuracy for k = 59: 43.00%\n",
      "Accuracy for k = 60: 43.00%\n",
      "Accuracy for k = 61: 43.00%\n",
      "Accuracy for k = 62: 44.00%\n",
      "Accuracy for k = 63: 44.00%\n",
      "Accuracy for k = 64: 44.00%\n",
      "Accuracy for k = 65: 43.00%\n",
      "Accuracy for k = 66: 44.00%\n",
      "Accuracy for k = 67: 44.00%\n",
      "Accuracy for k = 68: 44.00%\n",
      "Accuracy for k = 69: 44.00%\n",
      "Accuracy for k = 70: 44.00%\n",
      "Accuracy for k = 71: 44.00%\n",
      "Accuracy for k = 72: 43.00%\n",
      "Accuracy for k = 73: 42.00%\n",
      "Accuracy for k = 74: 43.00%\n",
      "\n",
      "Best k value:\n",
      "k = 62: Accuracy = 44.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier after LDA\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_lda, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75) \n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_lda[:num_test_samples], k=k)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "n_components = 100  # Set the number of components you want to keep after PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA on training data and transform both train and test data\n",
    "x_train_pca = pca.fit_transform(x_train_flat)\n",
    "x_test_pca = pca.transform(x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 38.00%\n",
      "Accuracy for k = 2: 38.00%\n",
      "Accuracy for k = 3: 38.00%\n",
      "Accuracy for k = 4: 43.00%\n",
      "Accuracy for k = 5: 42.00%\n",
      "Accuracy for k = 6: 41.00%\n",
      "Accuracy for k = 7: 44.00%\n",
      "Accuracy for k = 8: 44.00%\n",
      "Accuracy for k = 9: 44.00%\n",
      "Accuracy for k = 10: 38.00%\n",
      "Accuracy for k = 11: 46.00%\n",
      "Accuracy for k = 12: 46.00%\n",
      "Accuracy for k = 13: 45.00%\n",
      "Accuracy for k = 14: 47.00%\n",
      "Accuracy for k = 15: 49.00%\n",
      "Accuracy for k = 16: 50.00%\n",
      "Accuracy for k = 17: 48.00%\n",
      "Accuracy for k = 18: 46.00%\n",
      "Accuracy for k = 19: 48.00%\n",
      "Accuracy for k = 20: 48.00%\n",
      "Accuracy for k = 21: 47.00%\n",
      "Accuracy for k = 22: 49.00%\n",
      "Accuracy for k = 23: 50.00%\n",
      "Accuracy for k = 24: 52.00%\n",
      "Accuracy for k = 25: 50.00%\n",
      "Accuracy for k = 26: 50.00%\n",
      "Accuracy for k = 27: 49.00%\n",
      "Accuracy for k = 28: 51.00%\n",
      "Accuracy for k = 29: 50.00%\n",
      "Accuracy for k = 30: 50.00%\n",
      "Accuracy for k = 31: 51.00%\n",
      "Accuracy for k = 32: 50.00%\n",
      "Accuracy for k = 33: 51.00%\n",
      "Accuracy for k = 34: 50.00%\n",
      "Accuracy for k = 35: 48.00%\n",
      "Accuracy for k = 36: 50.00%\n",
      "Accuracy for k = 37: 50.00%\n",
      "Accuracy for k = 38: 48.00%\n",
      "Accuracy for k = 39: 46.00%\n",
      "Accuracy for k = 40: 49.00%\n",
      "Accuracy for k = 41: 49.00%\n",
      "Accuracy for k = 42: 49.00%\n",
      "Accuracy for k = 43: 47.00%\n",
      "Accuracy for k = 44: 48.00%\n",
      "Accuracy for k = 45: 49.00%\n",
      "Accuracy for k = 46: 50.00%\n",
      "Accuracy for k = 47: 48.00%\n",
      "Accuracy for k = 48: 48.00%\n",
      "Accuracy for k = 49: 48.00%\n",
      "Accuracy for k = 50: 49.00%\n",
      "Accuracy for k = 51: 49.00%\n",
      "Accuracy for k = 52: 50.00%\n",
      "Accuracy for k = 53: 49.00%\n",
      "Accuracy for k = 54: 50.00%\n",
      "Accuracy for k = 55: 48.00%\n",
      "Accuracy for k = 56: 48.00%\n",
      "Accuracy for k = 57: 46.00%\n",
      "Accuracy for k = 58: 46.00%\n",
      "Accuracy for k = 59: 47.00%\n",
      "Accuracy for k = 60: 48.00%\n",
      "Accuracy for k = 61: 46.00%\n",
      "Accuracy for k = 62: 46.00%\n",
      "Accuracy for k = 63: 46.00%\n",
      "Accuracy for k = 64: 46.00%\n",
      "Accuracy for k = 65: 45.00%\n",
      "Accuracy for k = 66: 43.00%\n",
      "Accuracy for k = 67: 43.00%\n",
      "Accuracy for k = 68: 44.00%\n",
      "Accuracy for k = 69: 42.00%\n",
      "Accuracy for k = 70: 45.00%\n",
      "Accuracy for k = 71: 46.00%\n",
      "Accuracy for k = 72: 47.00%\n",
      "Accuracy for k = 73: 47.00%\n",
      "Accuracy for k = 74: 46.00%\n",
      "\n",
      "Best k value:\n",
      "k = 24: Accuracy = 52.00%\n"
     ]
    }
   ],
   "source": [
    "# Flatten y_test if necessary\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Train the classifier after PCA\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_pca, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75)\n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_pca[:num_test_samples], k=k)  # Use x_test_pca here\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_features(images, kernels=None):\n",
    "    \"\"\"\n",
    "    Apply Gabor filters to a batch of images.\n",
    "    \n",
    "    Parameters:\n",
    "    - images: A batch of images of shape (N, H, W, C) where N is the number of images,\n",
    "      H is height, W is width, and C is the number of channels.\n",
    "    - kernels: A list of Gabor kernels to apply. If None, it generates a set of kernels.\n",
    "    \n",
    "    Returns:\n",
    "    - features: A 2D array of shape (N, F) where F is the number of features per image.\n",
    "    \"\"\"\n",
    "    if kernels is None:\n",
    "        kernels = generate_gabor_kernels()\n",
    "    \n",
    "    # Initialize a list to store the features\n",
    "    features = []\n",
    "    \n",
    "    for img in images:\n",
    "        # Ensure the image is grayscale (Gabor requires single-channel images)\n",
    "        if img.ndim == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        img_features = []\n",
    "        for kernel in kernels:\n",
    "            # Apply the Gabor filter\n",
    "            filtered = cv2.filter2D(img, cv2.CV_32F, kernel)\n",
    "            # Extract statistics as features (mean and standard deviation)\n",
    "            img_features.append(filtered.mean())\n",
    "            img_features.append(filtered.std())\n",
    "        \n",
    "        features.append(img_features)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def generate_gabor_kernels(scales=5, orientations=8):\n",
    "    \"\"\"\n",
    "    Generate Gabor filter kernels.\n",
    "    \n",
    "    Parameters:\n",
    "    - scales: Number of different scales (frequencies).\n",
    "    - orientations: Number of orientations for each scale.\n",
    "    \n",
    "    Returns:\n",
    "    - kernels: A list of Gabor kernels.\n",
    "    \"\"\"\n",
    "    kernels = []\n",
    "    for theta in np.linspace(0, np.pi, orientations, endpoint=False):  # Orientations\n",
    "        for sigma in (1, 2):  # Scale parameters\n",
    "            kernel = cv2.getGaborKernel(ksize=(9, 9), sigma=sigma, theta=theta, lambd=10.0, gamma=0.5, psi=0)\n",
    "            kernels.append(kernel)\n",
    "    return kernels\n",
    "\n",
    "def generate_features_in_batches(data, batch_size, feature_extractor):\n",
    "    \"\"\"\n",
    "    Generate features in batches to save memory.\n",
    "    :param data: Original data of shape (N, H, W, C).\n",
    "    :param batch_size: Batch size for feature computation.\n",
    "    :param feature_extractor: A callable that extracts features for a batch.\n",
    "    :return: Features array.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        print(f\"Processing batch {i // batch_size + 1}\", end=\"\\r\")\n",
    "        batch = data[i:i + batch_size]\n",
    "        batch_features = feature_extractor(batch)  # Apply feature extraction\n",
    "        features.append(batch_features)\n",
    "    return np.vstack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10\r"
     ]
    }
   ],
   "source": [
    "kernels = generate_gabor_kernels()\n",
    "\n",
    "x_train_gabor = generate_features_in_batches(\n",
    "    x_train, batch_size=1000, feature_extractor=lambda x: gabor_features(x, kernels)\n",
    ")\n",
    "x_test_gabor = generate_features_in_batches(\n",
    "    x_test, batch_size=1000, feature_extractor=lambda x: gabor_features(x, kernels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    return (features - features.mean(axis=0)) / features.std(axis=0)\n",
    "\n",
    "x_train_gabor = normalize_features(x_train_gabor)\n",
    "x_test_gabor = normalize_features(x_test_gabor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 18.00%\n",
      "Accuracy for k = 2: 18.00%\n",
      "Accuracy for k = 3: 18.00%\n",
      "Accuracy for k = 4: 20.00%\n",
      "Accuracy for k = 5: 18.00%\n",
      "Accuracy for k = 6: 17.00%\n",
      "Accuracy for k = 7: 19.00%\n",
      "Accuracy for k = 8: 18.00%\n",
      "Accuracy for k = 9: 18.00%\n",
      "Accuracy for k = 10: 22.00%\n",
      "Accuracy for k = 11: 21.00%\n",
      "Accuracy for k = 12: 20.00%\n",
      "Accuracy for k = 13: 21.00%\n",
      "Accuracy for k = 14: 21.00%\n",
      "Accuracy for k = 15: 23.00%\n",
      "Accuracy for k = 16: 25.00%\n",
      "Accuracy for k = 17: 24.00%\n",
      "Accuracy for k = 18: 24.00%\n",
      "Accuracy for k = 19: 23.00%\n",
      "Accuracy for k = 20: 24.00%\n",
      "Accuracy for k = 21: 24.00%\n",
      "Accuracy for k = 22: 24.00%\n",
      "Accuracy for k = 23: 23.00%\n",
      "Accuracy for k = 24: 23.00%\n",
      "Accuracy for k = 25: 25.00%\n",
      "Accuracy for k = 26: 26.00%\n",
      "Accuracy for k = 27: 26.00%\n",
      "Accuracy for k = 28: 25.00%\n",
      "Accuracy for k = 29: 27.00%\n",
      "Accuracy for k = 30: 26.00%\n",
      "Accuracy for k = 31: 26.00%\n",
      "Accuracy for k = 32: 28.00%\n",
      "Accuracy for k = 33: 27.00%\n",
      "Accuracy for k = 34: 27.00%\n",
      "Accuracy for k = 35: 25.00%\n",
      "Accuracy for k = 36: 27.00%\n",
      "Accuracy for k = 37: 25.00%\n",
      "Accuracy for k = 38: 26.00%\n",
      "Accuracy for k = 39: 26.00%\n",
      "Accuracy for k = 40: 27.00%\n",
      "Accuracy for k = 41: 28.00%\n",
      "Accuracy for k = 42: 28.00%\n",
      "Accuracy for k = 43: 28.00%\n",
      "Accuracy for k = 44: 28.00%\n",
      "Accuracy for k = 45: 28.00%\n",
      "Accuracy for k = 46: 28.00%\n",
      "Accuracy for k = 47: 29.00%\n",
      "Accuracy for k = 48: 28.00%\n",
      "Accuracy for k = 49: 27.00%\n",
      "Accuracy for k = 50: 28.00%\n",
      "Accuracy for k = 51: 28.00%\n",
      "Accuracy for k = 52: 28.00%\n",
      "Accuracy for k = 53: 27.00%\n",
      "Accuracy for k = 54: 27.00%\n",
      "Accuracy for k = 55: 28.00%\n",
      "Accuracy for k = 56: 29.00%\n",
      "Accuracy for k = 57: 29.00%\n",
      "Accuracy for k = 58: 29.00%\n",
      "Accuracy for k = 59: 29.00%\n",
      "Accuracy for k = 60: 29.00%\n",
      "Accuracy for k = 61: 29.00%\n",
      "Accuracy for k = 62: 28.00%\n",
      "Accuracy for k = 63: 29.00%\n",
      "Accuracy for k = 64: 28.00%\n",
      "Accuracy for k = 65: 27.00%\n",
      "Accuracy for k = 66: 27.00%\n",
      "Accuracy for k = 67: 27.00%\n",
      "Accuracy for k = 68: 27.00%\n",
      "Accuracy for k = 69: 28.00%\n",
      "Accuracy for k = 70: 28.00%\n",
      "Accuracy for k = 71: 28.00%\n",
      "Accuracy for k = 72: 28.00%\n",
      "Accuracy for k = 73: 28.00%\n",
      "Accuracy for k = 74: 28.00%\n",
      "\n",
      "Best k value:\n",
      "k = 47: Accuracy = 29.00%\n",
      "Final accuracy on the full test set: 26.72%\n"
     ]
    }
   ],
   "source": [
    "# Flatten y_test if necessary\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_gabor, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75)\n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_gabor[:num_test_samples], k=k)  # Use x_test_pca here\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")\n",
    "\n",
    "y_pred_full = knn.predict(x_test_gabor, k=best_k)\n",
    "final_accuracy = accuracy_score(y_test, y_pred_full)\n",
    "print(f\"Final accuracy on the full test set: {final_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phog(image):\n",
    "    \"\"\"\n",
    "    Compute the Pyramid Histogram of Oriented Gradients (PHOG) features for an image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image (2D grayscale or 3D color).\n",
    "\n",
    "    Returns:\n",
    "    - fd: Flattened feature descriptor for the image.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale if the image is in color\n",
    "    if image.ndim == 3:\n",
    "        image = rgb2gray(image)\n",
    "    \n",
    "    # Compute HOG features\n",
    "    fd = hog(\n",
    "        image, \n",
    "        orientations=9, \n",
    "        pixels_per_cell=(8, 8), \n",
    "        cells_per_block=(1, 1), \n",
    "        channel_axis=None  # Specify no channel for grayscale images\n",
    "    )\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_histogram_equalization(image):\n",
    "    \"\"\"\n",
    "    Apply histogram equalization to enhance image contrast.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image (2D grayscale or 3D color).\n",
    "\n",
    "    Returns:\n",
    "    - processed_image: Image after histogram equalization.\n",
    "    \"\"\"\n",
    "    if image.ndim == 3:  # Convert to grayscale if necessary\n",
    "        image = rgb2gray(image)\n",
    "    return equalize_hist(image)\n",
    "\n",
    "# Apply to dataset\n",
    "x_train_preprocessed = np.array([preprocess_histogram_equalization(img) for img in x_train])\n",
    "x_test_preprocessed = np.array([preprocess_histogram_equalization(img) for img in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_clahe(image):\n",
    "    \"\"\"\n",
    "    Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) for better local contrast.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image (2D grayscale or 3D color).\n",
    "\n",
    "    Returns:\n",
    "    - processed_image: Image after CLAHE.\n",
    "    \"\"\"\n",
    "    if image.ndim == 3:  # Convert to grayscale if necessary\n",
    "        image = rgb2gray(image)\n",
    "        image = (image * 255).astype(np.uint8)  # Convert to uint8 for CLAHE\n",
    "    \n",
    "    # Apply CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(image)\n",
    "\n",
    "# Apply to dataset\n",
    "x_train_preprocessed = np.array([preprocess_clahe(img) for img in x_train])\n",
    "x_test_preprocessed = np.array([preprocess_clahe(img) for img in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_intensity_normalization(image):\n",
    "    \"\"\"\n",
    "    Normalize pixel intensities to the range [0, 1].\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image (2D grayscale or 3D color).\n",
    "\n",
    "    Returns:\n",
    "    - processed_image: Normalized image.\n",
    "    \"\"\"\n",
    "    if image.ndim == 3:  # Convert to grayscale if necessary\n",
    "        image = rgb2gray(image)\n",
    "    return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "# Apply to dataset\n",
    "x_train_preprocessed = np.array([preprocess_intensity_normalization(img) for img in x_train])\n",
    "x_test_preprocessed = np.array([preprocess_intensity_normalization(img) for img in x_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_phog = np.array([compute_phog(img) for img in x_train_preprocessed])\n",
    "x_test_phog = np.array([compute_phog(img) for img in x_test_preprocessed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 28.00%\n",
      "Accuracy for k = 2: 28.00%\n",
      "Accuracy for k = 3: 33.00%\n",
      "Accuracy for k = 4: 35.00%\n",
      "Accuracy for k = 5: 35.00%\n",
      "Accuracy for k = 6: 34.00%\n",
      "Accuracy for k = 7: 31.00%\n",
      "Accuracy for k = 8: 35.00%\n",
      "Accuracy for k = 9: 34.00%\n",
      "Accuracy for k = 10: 33.00%\n",
      "Accuracy for k = 11: 34.00%\n",
      "Accuracy for k = 12: 31.00%\n",
      "Accuracy for k = 13: 32.00%\n",
      "Accuracy for k = 14: 34.00%\n",
      "Accuracy for k = 15: 34.00%\n",
      "Accuracy for k = 16: 36.00%\n",
      "Accuracy for k = 17: 33.00%\n",
      "Accuracy for k = 18: 31.00%\n",
      "Accuracy for k = 19: 34.00%\n",
      "Accuracy for k = 20: 36.00%\n",
      "Accuracy for k = 21: 33.00%\n",
      "Accuracy for k = 22: 33.00%\n",
      "Accuracy for k = 23: 34.00%\n",
      "Accuracy for k = 24: 34.00%\n",
      "Accuracy for k = 25: 34.00%\n",
      "Accuracy for k = 26: 34.00%\n",
      "Accuracy for k = 27: 33.00%\n",
      "Accuracy for k = 28: 32.00%\n",
      "Accuracy for k = 29: 32.00%\n",
      "Accuracy for k = 30: 34.00%\n",
      "Accuracy for k = 31: 34.00%\n",
      "Accuracy for k = 32: 34.00%\n",
      "Accuracy for k = 33: 32.00%\n",
      "Accuracy for k = 34: 31.00%\n",
      "Accuracy for k = 35: 31.00%\n",
      "Accuracy for k = 36: 31.00%\n",
      "Accuracy for k = 37: 33.00%\n",
      "Accuracy for k = 38: 32.00%\n",
      "Accuracy for k = 39: 32.00%\n",
      "Accuracy for k = 40: 32.00%\n",
      "Accuracy for k = 41: 32.00%\n",
      "Accuracy for k = 42: 32.00%\n",
      "Accuracy for k = 43: 32.00%\n",
      "Accuracy for k = 44: 32.00%\n",
      "Accuracy for k = 45: 32.00%\n",
      "Accuracy for k = 46: 31.00%\n",
      "Accuracy for k = 47: 33.00%\n",
      "Accuracy for k = 48: 32.00%\n",
      "Accuracy for k = 49: 32.00%\n",
      "Accuracy for k = 50: 31.00%\n",
      "Accuracy for k = 51: 31.00%\n",
      "Accuracy for k = 52: 32.00%\n",
      "Accuracy for k = 53: 31.00%\n",
      "Accuracy for k = 54: 31.00%\n",
      "Accuracy for k = 55: 31.00%\n",
      "Accuracy for k = 56: 32.00%\n",
      "Accuracy for k = 57: 32.00%\n",
      "Accuracy for k = 58: 32.00%\n",
      "Accuracy for k = 59: 34.00%\n",
      "Accuracy for k = 60: 33.00%\n",
      "Accuracy for k = 61: 32.00%\n",
      "Accuracy for k = 62: 32.00%\n",
      "Accuracy for k = 63: 32.00%\n",
      "Accuracy for k = 64: 32.00%\n",
      "Accuracy for k = 65: 32.00%\n",
      "Accuracy for k = 66: 32.00%\n",
      "Accuracy for k = 67: 32.00%\n",
      "Accuracy for k = 68: 33.00%\n",
      "Accuracy for k = 69: 32.00%\n",
      "Accuracy for k = 70: 32.00%\n",
      "Accuracy for k = 71: 34.00%\n",
      "Accuracy for k = 72: 33.00%\n",
      "Accuracy for k = 73: 33.00%\n",
      "Accuracy for k = 74: 32.00%\n",
      "\n",
      "Best k value:\n",
      "k = 16: Accuracy = 36.00%\n"
     ]
    }
   ],
   "source": [
    "# Flatten y_test if necessary\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Train the classifier after PCA\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_phog, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75)\n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_phog[:num_test_samples], k=k)  # Use x_test_pca here\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_gabor_pre shape: (50000, 32)\n",
      "x_test_gabor_pre shape: (10000, 32)\n"
     ]
    }
   ],
   "source": [
    "## WHAT IF PRE PROCESSING + GABOR + KNN \n",
    "\n",
    "x_train_preprocessed = np.array([preprocess_histogram_equalization(img) for img in x_train_flat])\n",
    "x_test_preprocessed = np.array([preprocess_histogram_equalization(img) for img in x_test_flat])\n",
    "\n",
    "x_train_gabor_pre = generate_features_in_batches(\n",
    "    x_train_preprocessed, batch_size=1000, feature_extractor=lambda x: gabor_features(x, kernels)\n",
    ")\n",
    "x_test_gabor_pre = generate_features_in_batches(\n",
    "    x_test_preprocessed, batch_size=1000, feature_extractor=lambda x: gabor_features(x, kernels)\n",
    ")\n",
    "\n",
    "print(f\"x_train_gabor_pre shape: {x_train_gabor_pre.shape}\")\n",
    "print(f\"x_test_gabor_pre shape: {x_test_gabor_pre.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 32  \n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "x_train_preprocessed_1 = np.array([preprocess_intensity_normalization(img) for img in x_train_flat])\n",
    "x_test_preprocessed_1 = np.array([preprocess_intensity_normalization(img) for img in x_test_flat])\n",
    "\n",
    "x_train_final = pca.fit_transform(x_train_preprocessed_1)\n",
    "x_test_final = pca.transform(x_test_preprocessed_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 42.00%\n",
      "Accuracy for k = 2: 42.00%\n",
      "Accuracy for k = 3: 46.00%\n",
      "Accuracy for k = 4: 44.00%\n",
      "Accuracy for k = 5: 51.00%\n",
      "Accuracy for k = 6: 47.00%\n",
      "Accuracy for k = 7: 47.00%\n",
      "Accuracy for k = 8: 50.00%\n",
      "Accuracy for k = 9: 50.00%\n",
      "Accuracy for k = 10: 44.00%\n",
      "Accuracy for k = 11: 50.00%\n",
      "Accuracy for k = 12: 47.00%\n",
      "Accuracy for k = 13: 46.00%\n",
      "Accuracy for k = 14: 48.00%\n",
      "Accuracy for k = 15: 50.00%\n",
      "Accuracy for k = 16: 50.00%\n",
      "Accuracy for k = 17: 49.00%\n",
      "Accuracy for k = 18: 50.00%\n",
      "Accuracy for k = 19: 49.00%\n",
      "Accuracy for k = 20: 49.00%\n",
      "Accuracy for k = 21: 49.00%\n",
      "Accuracy for k = 22: 50.00%\n",
      "Accuracy for k = 23: 50.00%\n",
      "Accuracy for k = 24: 51.00%\n",
      "Accuracy for k = 25: 52.00%\n",
      "Accuracy for k = 26: 49.00%\n",
      "Accuracy for k = 27: 48.00%\n",
      "Accuracy for k = 28: 47.00%\n",
      "Accuracy for k = 29: 48.00%\n",
      "Accuracy for k = 30: 46.00%\n",
      "Accuracy for k = 31: 46.00%\n",
      "Accuracy for k = 32: 47.00%\n",
      "Accuracy for k = 33: 47.00%\n",
      "Accuracy for k = 34: 46.00%\n",
      "Accuracy for k = 35: 47.00%\n",
      "Accuracy for k = 36: 50.00%\n",
      "Accuracy for k = 37: 50.00%\n",
      "Accuracy for k = 38: 50.00%\n",
      "Accuracy for k = 39: 51.00%\n",
      "Accuracy for k = 40: 51.00%\n",
      "Accuracy for k = 41: 52.00%\n",
      "Accuracy for k = 42: 50.00%\n",
      "Accuracy for k = 43: 49.00%\n",
      "Accuracy for k = 44: 49.00%\n",
      "Accuracy for k = 45: 50.00%\n",
      "Accuracy for k = 46: 50.00%\n",
      "Accuracy for k = 47: 49.00%\n",
      "Accuracy for k = 48: 48.00%\n",
      "Accuracy for k = 49: 48.00%\n",
      "Accuracy for k = 50: 47.00%\n",
      "Accuracy for k = 51: 47.00%\n",
      "Accuracy for k = 52: 46.00%\n",
      "Accuracy for k = 53: 47.00%\n",
      "Accuracy for k = 54: 48.00%\n",
      "Accuracy for k = 55: 47.00%\n",
      "Accuracy for k = 56: 48.00%\n",
      "Accuracy for k = 57: 47.00%\n",
      "Accuracy for k = 58: 46.00%\n",
      "Accuracy for k = 59: 47.00%\n",
      "Accuracy for k = 60: 48.00%\n",
      "Accuracy for k = 61: 48.00%\n",
      "Accuracy for k = 62: 48.00%\n",
      "Accuracy for k = 63: 48.00%\n",
      "Accuracy for k = 64: 47.00%\n",
      "Accuracy for k = 65: 46.00%\n",
      "Accuracy for k = 66: 45.00%\n",
      "Accuracy for k = 67: 46.00%\n",
      "Accuracy for k = 68: 46.00%\n",
      "Accuracy for k = 69: 46.00%\n",
      "Accuracy for k = 70: 46.00%\n",
      "Accuracy for k = 71: 45.00%\n",
      "Accuracy for k = 72: 43.00%\n",
      "Accuracy for k = 73: 42.00%\n",
      "Accuracy for k = 74: 44.00%\n",
      "\n",
      "Best k value:\n",
      "k = 25: Accuracy = 52.00%\n"
     ]
    }
   ],
   "source": [
    "# Flatten y_test if necessary\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "knn = kNearestNeighbour(metric='cosine')\n",
    "knn.train(x_train_final, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75)\n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_final[:num_test_samples], k=k)  # Use x_test_pca here\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNearestNeighbourKernel(object):\n",
    "    def __init__(self, metric='l1', kernel='uniform', bandwidth=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the KNN classifier.\n",
    "        :param metric: The distance metric to use ('l1', 'l2', 'cosine').\n",
    "        :param kernel: The kernel function to use ('uniform', 'gaussian', 'epanechnikov').\n",
    "        :param bandwidth: The bandwidth parameter for kernel functions.\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Memorize the training data.\n",
    "        :param X: Training data of shape (N, F).\n",
    "        :param Y: Training labels of shape (N,).\n",
    "        \"\"\"\n",
    "        self.Xtr = X\n",
    "        self.Ytr = Y\n",
    "\n",
    "    def _compute_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the distance between a single test example and all training examples.\n",
    "        :param x1: A single test example of shape (F,).\n",
    "        :param x2: Training examples of shape (N, F).\n",
    "        :return: Distance of shape (N,).\n",
    "        \"\"\"\n",
    "        if self.metric == 'l1':\n",
    "            return np.sum(np.abs(x2 - x1), axis=1)\n",
    "        elif self.metric == 'l2':\n",
    "            return np.sqrt(np.sum((x2 - x1) ** 2, axis=1))\n",
    "        elif self.metric == 'cosine':\n",
    "            x1_norm = np.linalg.norm(x1)\n",
    "            x2_norms = np.linalg.norm(x2, axis=1)\n",
    "            return 1 - (np.dot(x2, x1) / (x1_norm * x2_norms))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {self.metric}\")\n",
    "\n",
    "    def _compute_kernel(self, distances):\n",
    "        \"\"\"\n",
    "        Apply the kernel function to the distances.\n",
    "        :param distances: Distances of shape (N,).\n",
    "        :return: Kernel weights of shape (N,).\n",
    "        \"\"\"\n",
    "        if self.kernel == 'uniform':\n",
    "            return np.ones_like(distances)\n",
    "        elif self.kernel == 'gaussian':\n",
    "            return np.exp(-0.5 * (distances / self.bandwidth) ** 2)\n",
    "        elif self.kernel == 'epanechnikov':\n",
    "            weights = np.maximum(0, 1 - (distances / self.bandwidth) ** 2)\n",
    "            return weights\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported kernel: {self.kernel}\")\n",
    "\n",
    "    def predict(self, X, k):\n",
    "        \"\"\"\n",
    "        Predict labels for test data.\n",
    "        :param X: Test data of shape (M, F).\n",
    "        :param k: Number of neighbors to consider.\n",
    "        :return: Predicted labels of shape (M,).\n",
    "        \"\"\"\n",
    "        if k > self.Xtr.shape[0]:\n",
    "            raise ValueError(f\"k={k} is greater than the number of training samples={self.Xtr.shape[0]}\")\n",
    "\n",
    "        test_samples = X.shape[0]\n",
    "        Ypred = np.zeros(test_samples, dtype=self.Ytr.dtype)\n",
    "\n",
    "        for i in range(test_samples):\n",
    "            print(f\"Processing test sample {i + 1}/{test_samples}\", end=\"\\r\")\n",
    "\n",
    "            # Compute distances based on the chosen metric\n",
    "            dist = self._compute_distance(X[i, :], self.Xtr)\n",
    "\n",
    "            # Find the indices of the k smallest distances\n",
    "            idx = np.argpartition(dist, k)[:k]\n",
    "\n",
    "            # Compute kernel weights for the selected neighbors\n",
    "            neighbor_distances = dist[idx]\n",
    "            weights = self._compute_kernel(neighbor_distances)\n",
    "\n",
    "            # Weighted voting\n",
    "            label_count = np.zeros(10, dtype=np.float64)  # Assuming 10 classes\n",
    "            for j, x in enumerate(idx):\n",
    "                label_count[int(self.Ytr[x])] += weights[j]\n",
    "\n",
    "            # Assign the label with the maximum weighted vote\n",
    "            Ypred[i] = np.argmax(label_count)\n",
    "\n",
    "        return Ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1: 42.00%/100\n",
      "Accuracy for k = 2: 42.00%/100\n",
      "Accuracy for k = 3: 46.00%/100\n",
      "Accuracy for k = 4: 44.00%/100\n",
      "Accuracy for k = 5: 50.00%/100\n",
      "Accuracy for k = 6: 47.00%/100\n",
      "Accuracy for k = 7: 47.00%/100\n",
      "Accuracy for k = 8: 49.00%/100\n",
      "Accuracy for k = 9: 47.00%/100\n",
      "Accuracy for k = 10: 43.00%100\n",
      "Accuracy for k = 11: 48.00%100\n",
      "Accuracy for k = 12: 46.00%100\n",
      "Accuracy for k = 13: 45.00%100\n",
      "Accuracy for k = 14: 47.00%100\n",
      "Accuracy for k = 15: 49.00%100\n",
      "Accuracy for k = 16: 49.00%100\n",
      "Accuracy for k = 17: 49.00%100\n",
      "Accuracy for k = 18: 50.00%100\n",
      "Accuracy for k = 19: 47.00%100\n",
      "Accuracy for k = 20: 48.00%100\n",
      "Accuracy for k = 21: 47.00%100\n",
      "Accuracy for k = 22: 49.00%100\n",
      "Accuracy for k = 23: 48.00%100\n",
      "Accuracy for k = 24: 49.00%100\n",
      "Accuracy for k = 25: 51.00%100\n",
      "Accuracy for k = 26: 48.00%100\n",
      "Accuracy for k = 27: 46.00%100\n",
      "Accuracy for k = 28: 45.00%100\n",
      "Accuracy for k = 29: 46.00%100\n",
      "Accuracy for k = 30: 46.00%100\n",
      "Accuracy for k = 31: 46.00%100\n",
      "Accuracy for k = 32: 47.00%100\n",
      "Accuracy for k = 33: 47.00%100\n",
      "Accuracy for k = 34: 45.00%100\n",
      "Accuracy for k = 35: 45.00%100\n",
      "Accuracy for k = 36: 46.00%100\n",
      "Accuracy for k = 37: 49.00%100\n",
      "Accuracy for k = 38: 47.00%100\n",
      "Accuracy for k = 39: 48.00%100\n",
      "Accuracy for k = 40: 48.00%100\n",
      "Accuracy for k = 41: 50.00%100\n",
      "Accuracy for k = 42: 47.00%100\n",
      "Accuracy for k = 43: 47.00%100\n",
      "Accuracy for k = 44: 48.00%100\n",
      "Accuracy for k = 45: 47.00%100\n",
      "Accuracy for k = 46: 47.00%100\n",
      "Accuracy for k = 47: 46.00%100\n",
      "Accuracy for k = 48: 46.00%100\n",
      "Accuracy for k = 49: 48.00%100\n",
      "Accuracy for k = 50: 46.00%100\n",
      "Accuracy for k = 51: 46.00%100\n",
      "Accuracy for k = 52: 46.00%100\n",
      "Accuracy for k = 53: 47.00%100\n",
      "Accuracy for k = 54: 49.00%100\n",
      "Accuracy for k = 55: 49.00%100\n",
      "Accuracy for k = 56: 49.00%100\n",
      "Accuracy for k = 57: 47.00%100\n",
      "Accuracy for k = 58: 47.00%100\n",
      "Accuracy for k = 59: 48.00%100\n",
      "Accuracy for k = 60: 48.00%100\n",
      "Accuracy for k = 61: 49.00%100\n",
      "Accuracy for k = 62: 48.00%100\n",
      "Accuracy for k = 63: 48.00%100\n",
      "Accuracy for k = 64: 47.00%100\n",
      "Accuracy for k = 65: 46.00%100\n",
      "Accuracy for k = 66: 45.00%100\n",
      "Accuracy for k = 67: 46.00%100\n",
      "Accuracy for k = 68: 46.00%100\n",
      "Accuracy for k = 69: 46.00%100\n",
      "Accuracy for k = 70: 46.00%100\n",
      "Accuracy for k = 71: 45.00%100\n",
      "Accuracy for k = 72: 43.00%100\n",
      "Accuracy for k = 73: 43.00%100\n",
      "Accuracy for k = 74: 44.00%100\n",
      "\n",
      "Best k value:\n",
      "k = 25: Accuracy = 51.00%\n"
     ]
    }
   ],
   "source": [
    "# Flatten y_test if necessary\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "knn = kNearestNeighbourKernel(metric='cosine', kernel='epanechnikov', bandwidth=5)\n",
    "knn.train(x_train_final, y_train)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 75)\n",
    "\n",
    "# Store the accuracies for each k\n",
    "accuracies = []\n",
    "\n",
    "best_k = None\n",
    "highest_accuracy = 0\n",
    "\n",
    "for k in k_values:\n",
    "    # Predict the labels for a subset of test data\n",
    "    num_test_samples = 100  # To save time, use a small subset for testing\n",
    "    y_pred = knn.predict(x_test_final[:num_test_samples], k=k)  # Use x_test_pca here\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:num_test_samples], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if accuracy > highest_accuracy:\n",
    "        highest_accuracy = accuracy\n",
    "        best_k = k\n",
    "    \n",
    "    # Print accuracy for this k\n",
    "    print(f\"Accuracy for k = {k}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the best k and highest accuracy\n",
    "print(\"\\nBest k value:\")\n",
    "print(f\"k = {best_k}: Accuracy = {highest_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
